{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ads = pd.read_csv('final_testing_dataset.csv')\n",
    "\n",
    "# SELECT cc_text, ad FROM ads_nonads\n",
    "df_ads = df_ads[[\"cc_text\", \"ad\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             cc_text   ad\n",
      "0  creating havoc in our supply chains, and raisi...  1.0\n",
      "1  So he could charge rich tourists $12,500 for p...  1.0\n",
      "2  rock band foreign I'm Elissa Slotkin, and I'm ...  1.0\n",
      "3  In the meantime I think we have to provide the...  0.0\n",
      "4  And right now, we'll even pay off your phone w...  1.0\n",
      "(1009, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1009 entries, 0 to 1008\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   cc_text  1009 non-null   object \n",
      " 1   ad       1008 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_ads.head())\n",
    "print(df_ads.shape)\n",
    "print(df_ads.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2)\n",
      "(1007, 2)\n",
      "(1007, 2)\n"
     ]
    }
   ],
   "source": [
    "# drop rows with any missing values\n",
    "df_ads = df_ads.dropna()\n",
    "print(df_ads.shape)\n",
    "# drop duplicate rows\n",
    "df_ads = df_ads.drop_duplicates()\n",
    "print(df_ads.shape)\n",
    "# drop rows where 'ad' is not 0 or 1\n",
    "df_ads = df_ads[df_ads['ad'].isin([0, 1])]\n",
    "print(df_ads.shape)\n",
    "\n",
    "# Convert 'cc_text' column to string\n",
    "df_ads['cc_text'] = df_ads['cc_text'].astype(str)\n",
    "\n",
    "# Convert 'ad' column to integer\n",
    "df_ads['ad'] = df_ads['ad'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad\n",
      "0    509\n",
      "1    498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data check after cleaning\n",
    "print(df_ads[\"ad\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             cc_text  ad\n",
      "0  creating havoc in our supply chains, and raisi...   1\n",
      "1  So he could charge rich tourists $12,500 for p...   1\n",
      "2  rock band foreign I'm Elissa Slotkin, and I'm ...   1\n",
      "4  And right now, we'll even pay off your phone w...   1\n",
      "5  If you think you might be pregnant, you want t...   1\n",
      "                                              cc_text  ad\n",
      "3   In the meantime I think we have to provide the...   0\n",
      "14  them. anothern side, sam brown will be moving ...   0\n",
      "16  i'm bernie rayno join bri guy and myself at 6 ...   0\n",
      "25  apple is needed. just in this crew. on the sid...   0\n",
      "28  i'm bernie rayno join me and ariella is back o...   0\n"
     ]
    }
   ],
   "source": [
    "# print our the head of the data when ad is 1\n",
    "print(df_ads[df_ads[\"ad\"] == 1].head())\n",
    "\n",
    "# print our the head of the data when ad is 0\n",
    "print(df_ads[df_ads[\"ad\"] == 0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This dataset is relatively balanced now, so we do text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So he could charge rich tourists $12,500 for prime elk hunting. No wonder Sheehy said he'd end protections for public lands. So if you're for hunting and for access to public lands... ?you can't be for Shady Sheehy. Montana Outdoor Values Action Fund is responsible for the content of this ad.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ads[\"cc_text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And right now, we\\'ll even pay off your phone when you switch! ? (vo) For over 50 years Purina Cat Chow has been helping cats feel at home. With trusted nutrition, no wonder it\\'s the number one dry cat food in America. Come home to Cat Chow. ? (\"Ladies\\' Night By: Kool & the Gang) ? (?) (?) Get your grills out this summer with Pepsi, the official beverage of Grills Night Out.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ads[\"cc_text\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove everything within HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # lower case \n",
    "    text = text.lower()\n",
    "    # Remove special characters except for commas and periods\n",
    "    text = re.sub(r'[^a-z\\s,.]', '', text)\n",
    "    # Remove special characters at the beginning of the sentence\n",
    "    # text = re.sub(r'^[^A-Za-z0-9\\s]+', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so he could charge rich tourists , for prime elk hunting. no wonder sheehy said hed end protections for public lands. so if youre for hunting and for access to public lands... you cant be for shady sheehy. montana outdoor values action fund is responsible for the content of this ad.\n"
     ]
    }
   ],
   "source": [
    "sample_1 = clean_text(df_ads[\"cc_text\"][1])\n",
    "print(sample_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and right now, well even pay off your phone when you switch vo for over years purina cat chow has been helping cats feel at home. with trusted nutrition, no wonder its the number one dry cat food in america. come home to cat chow. ladies night by kool the gang get your grills out this summer with pepsi, the official beverage of grills night out.\n"
     ]
    }
   ],
   "source": [
    "sample_2 = clean_text(df_ads[\"cc_text\"][4])\n",
    "print(sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_text function to cc_text column\n",
    "df_ads[\"cc_text\"] = df_ads[\"cc_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    creating havoc in our supply chains, and raisi...\n",
       "1    so he could charge rich tourists , for prime e...\n",
       "2    rock band foreign im elissa slotkin, and im ru...\n",
       "3    in the meantime i think we have to provide the...\n",
       "4    and right now, well even pay off your phone wh...\n",
       "Name: cc_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ads[\"cc_text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (603,) (603,)\n",
      "Validation set size: (202,) (202,)\n",
      "Testing set size: (202,) (202,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df_ads[\"cc_text\"]\n",
    "y = df_ads[\"ad\"]\n",
    "\n",
    "# Step 1: Split the data into training+validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the training+validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Print the shapes of the resulting datasets to verify\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can use training dataset to build your model and text dataset to test the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once and restart the kernal \n",
    "#%pip install transformers[sentencepiece] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# checkpoint = \"bert-base-cased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# num_labels = 2  # 2 labels: 0 for non-ads, 1 for ads\n",
    "\n",
    "# model = (AutoModelForSequenceClassification\n",
    "#          .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "#          .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the training, testing, evaluation sets\n",
    "X_train_tokens = tokenizer(X_train.tolist(), padding=True, truncation=True)\n",
    "X_test_tokens = tokenizer(X_test.tolist(), padding=True, truncation=True)\n",
    "X_val_tokens = tokenizer(X_val.tolist(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Print the first few examples of tokenized training data\n",
    "print(X_train_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ad\n",
       "0    305\n",
       "1    298\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_seq = torch.tensor(X_train_tokens[\"input_ids\"])\n",
    "train_mask = torch.tensor(X_train_tokens[\"attention_mask\"])\n",
    "train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "val_seq = torch.tensor(X_val_tokens[\"input_ids\"])\n",
    "val_mask = torch.tensor(X_val_tokens[\"attention_mask\"])\n",
    "val_y = torch.tensor(y_val.tolist())\n",
    "\n",
    "test_seq = torch.tensor(X_test_tokens[\"input_ids\"])\n",
    "test_mask = torch.tensor(X_test_tokens[\"attention_mask\"])\n",
    "test_y = torch.tensor(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create instances of the CustomDataset\n",
    "train_dataset = CustomDataset(train_seq, train_mask, train_y)\n",
    "val_dataset = CustomDataset(val_seq, val_mask, val_y)\n",
    "test_dataset = CustomDataset(test_seq, test_mask, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataParallel if multiple GPUs are available\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(torch.cuda.device_count())\n",
    "#     model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2  # 2 labels: 0 for non-ads, 1 for ads\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "logging_steps = (len(train_dataset) // batch_size) \n",
    "logging_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "model_name = f\"{checkpoint}-adremoval_testingdata\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim='adamw_torch',\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_accuracy(preds):\n",
    "  accuracy = accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))\n",
    "  return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=get_accuracy,\n",
    "    args=training_args,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664ca06f5fc47d1afbf8ca29968ba3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5518, 'grad_norm': 5.558544635772705, 'learning_rate': 1.0263157894736844e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fff710f7be40f681dc23dcbacec58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4463832676410675, 'eval_accuracy': 0.8267326732673267, 'eval_runtime': 38.4375, 'eval_samples_per_second': 5.255, 'eval_steps_per_second': 0.338, 'epoch': 1.0}\n",
      "{'loss': 0.292, 'grad_norm': 5.64611291885376, 'learning_rate': 5.263157894736843e-07, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5674571529154eeda825634840f68ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31147944927215576, 'eval_accuracy': 0.8712871287128713, 'eval_runtime': 39.1658, 'eval_samples_per_second': 5.158, 'eval_steps_per_second': 0.332, 'epoch': 2.0}\n",
      "{'train_runtime': 834.2506, 'train_samples_per_second': 1.446, 'train_steps_per_second': 0.091, 'train_loss': 0.41747593879699707, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=76, training_loss=0.41747593879699707, metrics={'train_runtime': 834.2506, 'train_samples_per_second': 1.446, 'train_steps_per_second': 0.091, 'total_flos': 210714955351200.0, 'train_loss': 0.41747593879699707, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259d5ce229de40268dc6a5a34a29591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2973465025424957,\n",
       " 'eval_accuracy': 0.8861386138613861,\n",
       " 'eval_runtime': 54.8417,\n",
       " 'eval_samples_per_second': 3.683,\n",
       " 'eval_steps_per_second': 0.237,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased-adremoval_testingdata'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from another Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.88851398229599}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lable 1\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model=model_name)\n",
    "classifier('so he could charge rich tourists $12,500 for prime elk hunting. no wonder sheehy said he end protections for public lands. so if you re for hunting and for access to public lands... you cant be for shady. montana outdoor values action fund is responsible for the content of this ad.')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.8665247559547424}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lable 0 \n",
    "classifier('in the meantime i think we have to provide the studies necessary no thanks he s palestine residents say the study has a good first step but they also need medical care to go with it a resident still do not have also caller just a companion bill in the senate is being cosponsored by republican J. D. Vance and democrat sherrod brown to also help push the I. R. S. to announce wednesday the twenty one million dollars norfolk southern says it has paid directly to residents will not be taxed the norfolk southern do that there was a community the I. R. S. did damage on top of that and we fix that those who have already reported the payments on their twenty twenty three taxes will need to amend the returns to get a refund last month norfolk southern also agreed to pay three hundred ten million dollars for cleanup and other fees bringing its total expected costs related to the derailment to one point seven billion dollars in washington cary leahy spectrum news you number released this week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.8129271864891052}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label1\n",
    "classifier(\"Need to get your a1c down? You may pay as little as $10 per prescription. Why do vitamins and supplements cost so much more now? Other companies are charging you more and more for less and less, and we hate that. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
